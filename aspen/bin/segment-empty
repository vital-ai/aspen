#!/usr/bin/env groovy

import static groovy.io.FileType.FILES

def homeDir = new File(getClass().protectionDomain.codeSource.location.path).parentFile.parentFile.getAbsolutePath() + '/';

def mainClass = 'ai.vital.aspen.data.SegmentEmptyJob'

if ( ! System.getenv('SPARK_HOME') ) {
	System.err.println("No SPARK_HOME environment variable set!");
	return
}

def sparkhome = System.getenv('SPARK_HOME')

def sparkAssembly = new File(sparkhome, 'lib/spark-assembly-1.6.0-hadoop2.6.0.jar')
def datanucleusapijdo = new File(sparkhome, 'lib/datanucleus-api-jdo-3.2.6.jar')
def datanucleuscore = new File(sparkhome, 'lib/datanucleus-core-3.2.10.jar')
def datanucleusrdbms = new File(sparkhome, 'lib/datanucleus-rdbms-3.2.9.jar')


if ( ! sparkAssembly.exists() ) {
  System.err.println("Spark assemly jar not found: " + sparkAssembly.absolutePath);
  return
}

List jars = []

new File(homeDir, "target").eachFile(FILES) {
  if(it.name.matches("aspen\\-(jobs\\-)?\\d+\\.\\d+\\.\\d+\\.jar")) {
    jars.add(it.absolutePath)
  }
}

jars.add( sparkAssembly.absolutePath )
jars.add( datanucleusapijdo.absolutePath )
jars.add( datanucleuscore.absolutePath )
jars.add( datanucleusrdbms.absolutePath )


List cmd = ['java', '-cp', (String)jars.join(File.pathSeparator), mainClass];

for(String a : args) {
	cmd.add(a);
}

//println(cmd);

def process = new ProcessBuilder(cmd).redirectErrorStream(true).start()
process.inputStream.eachLine {println it}